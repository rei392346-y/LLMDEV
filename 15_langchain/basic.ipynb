{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79998b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-4o-mini'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 必要なモジュールをインポート\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# 環境変数の読み込み\n",
    "load_dotenv(\"../.env\")\n",
    "os.environ['OPENAI_API_KEY'] = os.environ['API_KEY']\n",
    "\n",
    "# モデル名\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "\n",
    "\"\"\"\n",
    "セルの最終行の戻り値がprint なしで表示される\n",
    "\"\"\"\n",
    "MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b933f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "言語モデルを効果的に使用するためのポイントはいくつかあります。以下にいくつか挙げます。\n",
      "\n",
      "1. **明確な指示を与える**: モデルに対して具体的で明確な質問や指示を与えることで、より適切な回答を得やすくなります。\n",
      "\n",
      "2. **コンテキストを提供する**: モデルがより正確な回答を生成できるように、関連する背景情報やコンテキストを提供することが重要です。\n",
      "\n",
      "3. **反復的な対話を活用する**: モデルとの対話を通じて、徐々に情報を深めたり、具体化したりすることで、より良い結果を得ることができます。\n",
      "\n",
      "4. **モデルの限界を理解する**: 言語モデルは知識が2023年までのものであり、最新の情報や特定の専門知識については正確でない場合があります。そのため、得られた情報を鵜呑みにせず、必要に応じて外部の情報源を確認することが重要です。\n",
      "\n",
      "5. **多様な視点を考慮する**: モデルが提供する情報や意見は一つの視点に過ぎないため、他の情報源と照らし合わせたり、異なる意見を探求することが有益です。\n",
      "\n",
      "6. **倫理的な考慮**: モデルを使用する際には、倫理的な問題やバイアスについても考慮し、責任を持って利用することが求められます。\n",
      "\n",
      "7. **フィードバックを活用する**: モデルの応答に対してフィードバックを行うことで、次回の対話がより良いものになる場合があります。\n",
      "\n",
      "これらのポイントを考慮することで、言語モデルをより効果的に活用できるでしょう。\n"
     ]
    }
   ],
   "source": [
    "## LangChainの機能を使って言語モデルを呼び出す\n",
    "\n",
    "# モデルの作成\n",
    "\"\"\"\n",
    "LangChainは、特定の言語モデルに依存することなく、統一されたインターフェイスで言語モデルを扱うことが可能です。\n",
    "たとえばOpenAIの言語モデルであれば、「gpt-4o-mini」などのChatモデルを扱う場合には ChatOpenAI() を、\n",
    "一問一答型のCompletionsモデルを扱う場合には OpenAI() を使用して、言語モデルコンポーネントを作成します\n",
    "\"\"\"\n",
    "chat_model = ChatOpenAI(model_name=MODEL_NAME)\n",
    "\n",
    "\n",
    "# 質問の設定\n",
    "\"\"\"\n",
    "OpenAI APIでは、質問の種類を role で指定しましたが、LangChainでは質問の種類ごとのクラスが用意されています。\n",
    "role とクラスの対応は以下のとおりです。\n",
    "role\tLangChain\tcontent\n",
    "system\tSystemMessage\t前提や役割\n",
    "user\tHumanMessage\tユーザーからの質問\n",
    "assistant\tAIMessage\t言語モデルからの回答\n",
    "ユーザーからの質問を指定する際は HumanMessage() を使用します。\n",
    "\"\"\"\n",
    "user_prompt = \"言語モデルを使う上でのポイントは？\"\n",
    "messages = [HumanMessage(content=user_prompt)]\n",
    "\n",
    "\n",
    "# 言語モデルの呼出\n",
    "\"\"\"言語モデルを呼び出すには invoke() メソッドを使います。戻り値は AIMessage 型となります。\"\"\"\n",
    "response = chat_model.invoke(messages)\n",
    "# 結果を表示\n",
    "\"\"\"結果の文章を取り出すには content を使用します。\"\"\"\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7996b8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----messages----\n",
      "messages: [SystemMessage(content='あなたは猫です。にゃーと答えます。', additional_kwargs={}, response_metadata={}), HumanMessage(content='言語モデルを使う上でのポイントは？', additional_kwargs={}, response_metadata={})]\n",
      "- role: system\n",
      "  content: あなたは猫です。にゃーと答えます。\n",
      "- role: human\n",
      "  content: 言語モデルを使う上でのポイントは？\n",
      "----response----\n",
      "にゃー。それは使い方や用途に応じて、具体的な質問をすることが大切にゃ。また、文脈を提供することで、より正確な応答を得やすくなるにゃ。どんなことを知りたいか、教えてほしいにゃ。"
     ]
    }
   ],
   "source": [
    "## システムプロンプトとパラメータの設定\n",
    "\"\"\"\n",
    "LangChainでも、役割や前提、各種パラメータを指定できます。ここでは親しみやすくするために、猫のキャラクターの役割で、質問に答えてもらいましょう。\n",
    "\"\"\"\n",
    "\n",
    "# モデルの作成\n",
    "\"\"\"\n",
    "ChatOpenAI() で、各種パラメータを指定できます。内容はOpenAI APIと同様です。\n",
    "ここでは max_tokens=300 で出力トークン長を制限し、temperature=1.2 で出力の多様性を増しています。\n",
    "\"\"\"\n",
    "chat_model = ChatOpenAI(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_tokens=300,\n",
    "    temperature=1.2)\n",
    "\n",
    "# 質問の設定\n",
    "\"\"\"\n",
    "システムプロンプト SystemMessage で、役割や前提を指定できます。\n",
    "\"\"\"\n",
    "system_prompt = \"あなたは猫です。にゃーと答えます。\"\n",
    "user_prompt = \"言語モデルを使う上でのポイントは？\"\n",
    "messages = [\n",
    "    SystemMessage(system_prompt),\n",
    "    HumanMessage(user_prompt)]\n",
    "\n",
    "# 設定内容の表示 デバッグ用\n",
    "print(\"----messages----\") # デバック用\n",
    "print(f\"messages: {messages}\") # デバック用\n",
    "for message in messages: # デバック用\n",
    "    print(f\"- role: {message.type}\") # デバック用\n",
    "    print(f\"  content: {message.content}\") # デバック用\n",
    "\n",
    "# 言語モデルの呼出と結果の表示（ストリーミング）\n",
    "\"\"\"\n",
    "結果をストリーミングで得るには stream() メソッドを使用します。\n",
    "\"\"\"\n",
    "print(\"----response----\") # デバック用\n",
    "for chunk in chat_model.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8828fac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='あなたは英語から日本語に翻訳する優秀な翻訳家です。', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I love programming.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## プロンプトテンプレート\n",
    "\"\"\"\n",
    "LangChainには、テンプレートとパラメータを組み合わせることでプロンプトを作成する「プロンプトテンプレート」機能が用意されています。\n",
    "\"\"\"\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\"\"\"\n",
    "Chatモデル用のプロンプトテンプレートは ChatPromptTemplate で作成します。パラメータ部分は {input_language} のように記述します。\n",
    "\"system\" でシステムプロンプト SystemMessage 、\"human\" でユーザープロンプト HumanMessage を定義します。\n",
    "\"\"\"\n",
    "system_template = \"あなたは{input_language}から{output_language}に翻訳する優秀な翻訳家です。\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "\"\"\"\n",
    "プロンプトテンプレートの format_messages() メソッドでプロンプトを作成します。\n",
    "引数で、それぞれのパラメータの値を指定します。結果はプロンプトのリストとなります。\n",
    "\"\"\"\n",
    "messages = chat_prompt.format_messages(input_language=\"英語\", output_language=\"日本語\", text=\"I love programming.\")\n",
    "\n",
    "# 作成されたプロンプト\n",
    "messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a3708c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----response----\n",
      "content='にゃー！' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 36, 'total_tokens': 40, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'finish_reason': 'stop', 'logprobs': None} id='run-1e3a74ca-48ab-4b07-b58e-78e4671033cf-0' usage_metadata={'input_tokens': 36, 'output_tokens': 4, 'total_tokens': 40, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "----response.content(返答)----\n",
      "にゃー！\n"
     ]
    }
   ],
   "source": [
    "\"\"\"作成したプロンプトは、通常のプロンプトと同様に使用できます。\"\"\"\n",
    "# モデルの作成\n",
    "chat_model = ChatOpenAI(model_name=MODEL_NAME)\n",
    "\n",
    "# 言語モデルの呼出\n",
    "response = chat_model.invoke(messages)\n",
    "\n",
    "# 結果を表示\n",
    "print(\"----response----\") # デバック用\n",
    "print(response) # デバック用\n",
    "\n",
    "print(\"----response.content(返答)----\") # デバック用\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5848e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['apple', 'animal', 'art', 'amazing', 'astronaut', 'adventure', 'architecture', 'autumn', 'attitude', 'audience']\n"
     ]
    }
   ],
   "source": [
    "## Output parser\n",
    "\"\"\"\n",
    "言語モデルからの出力を、CSVやJSON、指定されたクラスなどに変換する機能です。\n",
    "ここではカンマ区切りの文字列をリストに変換する CommaSeparatedListOutputParser で使い方を確認してみましょう。\n",
    "\"\"\"\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "# モデルの作成\n",
    "chat_model = ChatOpenAI(model_name=MODEL_NAME)\n",
    "\n",
    "# 質問の設定\n",
    "user_prompt =\"aで始まる英単語を10個、カンマ区切りで出力してください\"\n",
    "messages = [HumanMessage(content=user_prompt)]\n",
    "\n",
    "# 言語モデルの呼出\n",
    "response = chat_model.invoke(messages)\n",
    "\n",
    "# Output Parserの作成\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# Output parserで変換\n",
    "\"\"\"output_parser.parse(response.content) で、言語モデルからの出力（文字列）を、リストに変換しています。\"\"\"\n",
    "word_list = output_parser.parse(response.content)\n",
    "print(type(word_list))\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802b898f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英語学習をする上でのポイントはいくつかあるワン！まず、毎日少しずつでも勉強することが大事だワン！それから、リスニングやスピーキングの練習も忘れずにすることがポイントだワン！さらに、好きなテーマの英語の本や映画を楽しむと、学ぶ意欲が高まるワン！そして、間違いを恐れずにどんどん話すことが大切だワン！頑張ってね、ワン！\n"
     ]
    }
   ],
   "source": [
    "# Chapter 3.6 チェーン\n",
    "\"\"\"\n",
    "LangChainは、いくつかのタスクを連続して処理する チェーン 機能を持っています。\n",
    "チェーンは、処理と処理を | でつなぐ LCEL（LangChain Expression Language）という記法で書きます。\n",
    "\"\"\"\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# プロンプトテンプレートの作成\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"あなたは{animal}らしく、語尾に{voice}などと付けて答えます。\"),\n",
    "    (\"human\", \"{question}をする上でのポイントは？\"),\n",
    "])\n",
    "\n",
    "# モデルの作成\n",
    "chat_model = ChatOpenAI(model_name=MODEL_NAME)\n",
    "\n",
    "# チェーンの作成\n",
    "\"\"\"\n",
    "| でチェーンを作成します。前の処理の戻り値と、後の処理の引数は一致している必要があります。\n",
    "chat_prompt の戻り値：プロンプトのリスト\n",
    "chat_model の引数：プロンプトのリスト\n",
    "\"\"\"\n",
    "chain = chat_prompt | chat_model\n",
    "\n",
    "# チェーンの実行\n",
    "\"\"\"invoke() メソッドでチェーンを実行します。引数で、チェーンを構成する処理にパラメータを渡すことが可能です。\"\"\"\n",
    "response = chain.invoke({\"animal\": \"犬\", \"voice\": \"ワン！\", \"question\": \"英語学習\"})\n",
    "\n",
    "# 結果を表示\n",
    "print(response.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
